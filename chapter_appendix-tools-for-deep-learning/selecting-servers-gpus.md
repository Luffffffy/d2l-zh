# 选择服务器和 GPU
:label:`sec_buy_gpu`

深度学习训练通常需要大量的计算。目前，GPU 是用于深度学习的最具成本效益的硬件加速器。特别是，与 CPU 相比，GPU 更便宜，性能更高，通常超过一个数量级。此外，一台服务器可以支持多个 GPU，高端服务器最多可支持 8 个 GPU。更典型的数字是工程工作站最多 4 个 GPU，因为散热、冷却和电源需求迅速升级，超出了办公楼的支持范围。对于大型部署，云计算（例如亚马逊的 [P3](https://aws.amazon.com/ec2/instance-types/p3/) 和 [G4](https://aws.amazon.com/blogs/aws/in-the-works-ec2-instances-g4-with-nvidia-t4-gpus/) 实例）是一种更实用的解决方案。 

## 选择服务器

通常不需要购买具有许多线程的高端 CPU，因为大部分计算都是在 GPU 上进行的。也就是说，由于 Python 中的全局解释器锁定 (GIL)，在我们有 4-8 个 GPU 的情况下，CPU 的单线程性能可能很重要。所有条件都相同，这表明具有较少内核数但时钟频率较高的 CPU 可能是更经济的选择。例如，在 6 核 4 GHz 和 8 核 3.5 GHz CPU 之间进行选择时，前者更可取，即使其总速度较低。一个重要的考虑因素是 GPU 会消耗大量功率，因此会散发大量热量。这需要非常好的散热和足够大的机箱来使用 GPU。如果可能，请遵循以下准则： 

1. ** 电源 **。GPU 会消耗大量功率。每台设备最高可达 350W 的预算（检查显卡的 * 峰值需求 * 而不是典型需求，因为高效的代码会消耗大量能源）。如果你的电源不能满足需求，你会发现你的系统变得不稳定。
1. ** 底盘尺寸 **。GPU 很大，辅助电源连接器通常需要额外的空间。此外，大型机箱更容易冷却。
1. **GPU 散热 **。如果你有大量的 GPU，你可能想投资水冷。另外，即使风扇较少，也要考虑使用 * 参考设计 *，因为它们足够薄，可以在设备之间进气。如果您购买多风扇 GPU，安装多个 GPU 时它可能太厚而无法获得足够的空气，并且会遇到散热限制。
1. **PCIe 插槽 **。在 GPU 之间移动数据（以及在 GPU 之间进行交换）需要大量带宽。我们建议使用 16 个通道的 PCIe 3.0 插槽。如果安装多个 GPU，请务必仔细阅读主板说明，以确保在同时使用多个 GPU 时 16$\times$ 带宽仍然可用，并确保增加插槽的是 PCIe 3.0 而不是 PCIe 2.0。在安装了多个 GPU 的情况下，某些主板会降级到 8$\times$ 甚至 4$\times$ 带宽。这部分是由于 CPU 提供的 PCIe 通道数量。

简而言之，以下是构建深度学习服务器的一些建议： 

* ** 初学者 **。购买低功耗的低端 GPU（适用于深度学习的廉价游戏 GPU 使用 150-200W）。如果幸运的话，您当前的计算机将支持它。
* **1 GPU**。具有 4 个内核的低端 CPU 就足够了，大多数主板就足够了。目标是至少 32 GB 的 DRAM，并投资购买用于本地数据访问的固态硬盘。600W 的电源应该足够了。购买有很多粉丝的 GPU。
* **2 个 GPU**。拥有 4-6 个内核的低端 CPU 就足够了。瞄准 64 GB DRAM 并投资购买固态硬盘。两个高端 GPU 需要 1000 瓦的功率。在主板方面，请确保它们有 * 两个 * PCIe 3.0 x16 插槽。如果可以的话，购买一个在 PCIe 3.0 x16 插槽之间有两个可用空间（60mm 间距）的主板，以增加空气。在这种情况下，购买两个有很多粉丝的 GPU。
* **4 个 GPU **。确保购买单线程速度相对较快（即高时钟频率）的 CPU。您可能需要一个具有更多 PCIe 通道的 CPU，例如 AMD Threadripper。您可能需要相对昂贵的主板才能获得 4 个 PCIe 3.0 x16 插槽，因为它们可能需要 PLX 才能对 PCIe 通道进行多路复用。购买参考设计狭窄的 GPU，让空气进入 GPU 之间。你需要一个 1600—2000W 的电源，而你办公室的插座可能不支持该电源。这台服务器可能会运行 * 响亮而热 *。你不想在办公桌下使用它。建议使用 128 GB 的 DRAM。获取用于本地存储的 SSD（1—2 TB NVMe）和一堆 RAID 配置的硬盘来存储您的数据。
* **8 个 GPU**。您需要购买具有多个冗余电源的专用多 GPU 服务器机箱（例如，2+1，每个电源 1600 瓦）。这将需要双插槽服务器 CPU、256 GB ECC DRAM、快速网卡（建议使用 10 GBE），并且您需要检查服务器是否支持 GPU 的 * 物理外形 *。消费者和服务器 GPU 之间的气流和布线位置差异很大（例如，RTX 2080 与 Tesla V100）。这意味着，由于电源线间隙不足或缺少合适的线束（正如其中一位合著者痛苦地发现的那样），您可能无法在服务器上安装消费者 GPU。

## 选择 GPU

目前，AMD 和 NVIDIA 是专用 GPU 的两家主要制造商。NVIDIA 是第一个进入深度学习领域的公司，通过 CUDA 为深度学习框架提供了更好的支持。因此，大多数买家选择 NVIDIA GPU。 

NVIDIA 提供两种类型的 GPU，针对个人用户（例如，通过 GTX 和 RTX 系列）和企业用户（通过其特斯拉系列）。这两种类型的 GPU 提供了相当的计算能力。但是，企业用户 GPU 通常使用（被动）强制冷却、更多内存和 ECC（纠错）内存。这些 GPU 更适合用于数据中心，价格通常是消费类 GPU 的十倍。 

如果您是一家拥有 100 多台服务器的大型公司，则应考虑使用 NVIDIA Tesla 系列，或者在云中使用 GPU 服务器。对于拥有 10 台以上服务器的实验室或中小型公司而言，NVIDIA RTX 系列可能最具成本效益。您可以购买带有 Supermicro 或 Asus 机箱的预配置服务器，这些机箱可有效容纳 4-8 个 GPU。 

GPU 供应商通常每隔一到两年发布一次新一代产品，例如 2017 年发布的 GTX 1000（帕斯卡）系列和 2019 年发布的 RTX 2000（图灵）系列。每个系列都提供几种不同的型号，提供不同的性能水平GPU 性能主要是以下三个参数的组合： 

1. ** 计算能力 **。一般来说，我们寻求 32 位浮点计算能力。16 位浮点训练 (FP16) 也正在进入主流。如果你只对预测感兴趣，你也可以使用 8 位整数。最新一代的图灵 GPU 提供 4 位加速。不幸的是，目前训练低精度网络的算法尚未普及。
1. ** 内存大小 **。随着模型变大或训练期间使用的批量增加，您将需要更多的 GPU 内存。检查 HBM2（高带宽内存）与 GDDR6（图形 DDR）内存。HBM2 速度更快，但昂贵得多。
1. ** 内存带宽 **。只有当你有足够的内存带宽时，你才能最大限度地利用你的计算能力。如果使用 GDDR6，请查找宽内存总线。

对于大多数用户来说，只要看一下计算能力就足够了。请注意，许多 GPU 提供不同类型的加速。例如，英伟达的 TensorCore 可以将一部分运营商的速度提高 5$\times$。确保你的图书馆支持这一点。GPU 内存应不小于 4 GB（8 GB 要好得多）。尽量避免同时使用 GPU 来显示 GUI（而是使用内置图形）。如果无法避免，为了安全起见，请额外添加 2 GB 的 RAM。 

:numref:`fig_flopsvsprice` 比较了各种 GTX 900、GTX 1000 和 RTX 2000 系列机型的 32 位浮点计算能力和价格。价格是维基百科上的建议价格。 

![Floating-point compute power and price comparison. ](../img/flopsvsprice.svg)
:label:`fig_flopsvsprice`

我们可以看到很多东西： 

1. 在每个系列中，价格和性能大致成比例。为了获得更大量的 GPU 内存，Titan 模型具有巨大的溢价。但是，通过比较 980 Ti 和 1080 Ti 可以看出，较新的型号具有更好的成本效益。RTX 2000 系列的价格似乎没有太大改善。但是，这是因为它们提供了远为优越的低精度性能（FP16、INT8 和 INT4）。
2. GTX 1000 系列的性能成本比大约是 900 系列的两倍。
3. 对于 RTX 2000 系列，价格是价格的 * 亲和 * 函数。

![Floating-point compute power and energy consumption. ](../img/wattvsprice.svg)
:label:`fig_wattvsprice`

:numref:`fig_wattvsprice` 显示了能耗如何随计算量呈线性变化。其次，后代的效率更高。这似乎与 RTX 2000 系列相对应的图表相矛盾。但是，这是 TensorCore 消耗不成比例的能量的结果。 

## 摘要

* 构建服务器时要注意电源、PCIe 总线通道、CPU 单线程速度和冷却。
* 如果可能的话，你应该购买最新一代的 GPU。
* 使用云进行大型部署。
* 高密度服务器可能并非与所有 GPU 兼容。购买前请检查机械和冷却规格。
* 使用 FP16 或更低的精度实现高效率。

[Discussions](https://discuss.d2l.ai/t/425)
